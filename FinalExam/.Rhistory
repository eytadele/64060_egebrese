predictTestTwit3 <- predict(Model_glm3, Test_twitter, type = "response")
preictedClasses3 <- as.factor(ifelse(predictTestTwit2 > 0.5,1,0))
print(confusionMatrix(preictedClasses3,Test_twitter$party))
gc()
gc()
library(dplyr)
library(tidyverse)
library(factoextra)
library(cluster)
library(gridExtra)
library(flexclust)
library(caret)
twtr <- read.csv("twitter_data_01.csv")
# convert party feature to factor
twtr$party <- as.factor(twtr$party)
twtr1 <- twtr[sample(nrow(twtr), 5000),]
twtr1 <- twtr1[,c(4,5,6,8)]
xtabs(~party,data = twtr1)
#summary(twtr.km)
set.seed(420)
# Model for partitioning twtr dataset into training and test data (70-30 split)
partModel <- createDataPartition(twtr1$party, p=0.7, list = FALSE)
# Creating the training and test datasets by applying the model
twTrain <- twtr1[partModel,]
twTest <- twtr1[-partModel,]
# create the logistic regression model
glmModel <- glm(party ~ . , family = "binomial", data = twTrain, maxit = 100)
summary(glmModel)
# prediction on the training set
predictTw <- predict(glmModel, twTrain, type = "response")
# assigning cutoff values
predictTw <- as.factor(ifelse(predictTw > 0.5,1,0))
print(confusionMatrix(predictTw,twTrain$party))
# prediction on the test set
predictTwTest <- predict(glmModel, twTest, type = "response")
# assigning cutoff values
predictTwTest <- as.factor(ifelse(predictTwTest > 0.5,1,0))
print(confusionMatrix(predictTwTest,twTest$party))
set.seed(420)
#twtr0 <- twtr[sample(nrow(twtr), 5000),]
twtr.km <- twtr1[,c(1,2,3)]
twtr.km <- scale(twtr.km) # scaling the dataset using z-score
#distance <- get_dist(twtr.km, method = "euclidean")
#fviz_dist(distance, gradient = list(low="dark green", mid="white", high="red"))
#head(round(as.matrix(distance),2),4) # displaying the first four rows rounded to 2 decimal places
set.seed(420)
# applying the Silhouette method using the fviz_nbclust function
fviz_nbclust(twtr.km, kmeans, method = "silhouette")
set.seed(420)
k2 <- kmeans(twtr.km, centers = 2, nstart = 50)
str(k2)
table(k2$cluster)
fviz_cluster(k2, data = twtr.km, pointsize = 1, labelsize = 7)
#incorporating the clusters with the non selected features and append to dataset
temp <- as.data.frame(list(k2$cluster))
twtr1$cluster <- temp[,c(1)]
twtr1$cluster <- as.factor(twtr1$cluster)
twtr1[,5] <- as.numeric(as.character(twtr1[,5]))
set.seed(420)
# Model for partitioning twtr dataset into training and test data (70-30 split)
T_idx <- createDataPartition(twtr1$party, p=0.7, list = FALSE)
# Creating the training and test datasets by applying the model
Train_twitter <- twtr1[T_idx,]
Test_twitter <- twtr1[-T_idx,]
summary(Train_twitter)
summary(Test_twitter)
# create the logistic regression model
Model_glm <- glm(party ~ . , family = "binomial", data = Train_twitter, maxit = 100)
# prediction on the test set
predictTestTwit <- predict(Model_glm, Test_twitter, type = "response")
# assigning cutoff values
preictedClasses <- as.factor(ifelse(predictTestTwit > 0.5,1,0))
# generating a confusion matrix and related details from the predictions
# made by the model
print(confusionMatrix(preictedClasses,Test_twitter$party))
# looking at ANOVA result or the model itself, we can see that retweets do not
# have a significant influence on the prediction; dropping it may give us a
# better predictive model maybe justify it
Model_glm2 <- glm(party ~ replies+retweets+cluster , family = "binomial", data = Train_twitter, maxit = 100)
predictTestTwit2 <- predict(Model_glm2, Test_twitter, type = "response")
preictedClasses2 <- as.factor(ifelse(predictTestTwit2 > 0.5,1,0))
print(confusionMatrix(preictedClasses2,Test_twitter$party))
Model_glm3 <- glm(party ~ replies+retweets , family = "binomial", data = Train_twitter, maxit = 100)
predictTestTwit3 <- predict(Model_glm3, Test_twitter, type = "response")
preictedClasses3 <- as.factor(ifelse(predictTestTwit2 > 0.5,1,0))
print(confusionMatrix(preictedClasses3,Test_twitter$party))
gc()
library(dplyr)
library(tidyverse)
library(factoextra)
library(cluster)
library(gridExtra)
library(flexclust)
library(caret)
twtr <- read.csv("twitter_data_01.csv")
# convert party feature to factor
twtr$party <- as.factor(twtr$party)
twtr1 <- twtr[sample(nrow(twtr), 10000),]
twtr1 <- twtr1[,c(4,5,6,8)]
xtabs(~party,data = twtr1)
#summary(twtr.km)
set.seed(420)
# Model for partitioning twtr dataset into training and test data (70-30 split)
partModel <- createDataPartition(twtr1$party, p=0.7, list = FALSE)
# Creating the training and test datasets by applying the model
twTrain <- twtr1[partModel,]
twTest <- twtr1[-partModel,]
# create the logistic regression model
glmModel <- glm(party ~ . , family = "binomial", data = twTrain, maxit = 100)
summary(glmModel)
# prediction on the training set
predictTw <- predict(glmModel, twTrain, type = "response")
# assigning cutoff values
predictTw <- as.factor(ifelse(predictTw > 0.5,1,0))
print(confusionMatrix(predictTw,twTrain$party))
# prediction on the test set
predictTwTest <- predict(glmModel, twTest, type = "response")
# assigning cutoff values
predictTwTest <- as.factor(ifelse(predictTwTest > 0.5,1,0))
print(confusionMatrix(predictTwTest,twTest$party))
set.seed(420)
#twtr0 <- twtr[sample(nrow(twtr), 5000),]
twtr.km <- twtr1[,c(1,2,3)]
twtr.km <- scale(twtr.km) # scaling the dataset using z-score
#distance <- get_dist(twtr.km, method = "euclidean")
#fviz_dist(distance, gradient = list(low="dark green", mid="white", high="red"))
#head(round(as.matrix(distance),2),4) # displaying the first four rows rounded to 2 decimal places
set.seed(420)
# applying the Silhouette method using the fviz_nbclust function
fviz_nbclust(twtr.km, kmeans, method = "silhouette")
set.seed(420)
k2 <- kmeans(twtr.km, centers = 2, nstart = 50)
str(k2)
table(k2$cluster)
fviz_cluster(k2, data = twtr.km, pointsize = 1, labelsize = 7)
#incorporating the clusters with the non selected features and append to dataset
temp <- as.data.frame(list(k2$cluster))
twtr1$cluster <- temp[,c(1)]
twtr1$cluster <- as.factor(twtr1$cluster)
twtr1[,5] <- as.numeric(as.character(twtr1[,5]))
set.seed(420)
# Model for partitioning twtr dataset into training and test data (70-30 split)
T_idx <- createDataPartition(twtr1$party, p=0.7, list = FALSE)
# Creating the training and test datasets by applying the model
Train_twitter <- twtr1[T_idx,]
Test_twitter <- twtr1[-T_idx,]
summary(Train_twitter)
summary(Test_twitter)
# create the logistic regression model
Model_glm <- glm(party ~ . , family = "binomial", data = Train_twitter, maxit = 100)
# prediction on the test set
predictTestTwit <- predict(Model_glm, Test_twitter, type = "response")
# assigning cutoff values
preictedClasses <- as.factor(ifelse(predictTestTwit > 0.5,1,0))
# generating a confusion matrix and related details from the predictions
# made by the model
print(confusionMatrix(preictedClasses,Test_twitter$party))
# looking at ANOVA result or the model itself, we can see that retweets do not
# have a significant influence on the prediction; dropping it may give us a
# better predictive model maybe justify it
Model_glm2 <- glm(party ~ replies+retweets+cluster , family = "binomial", data = Train_twitter, maxit = 100)
predictTestTwit2 <- predict(Model_glm2, Test_twitter, type = "response")
preictedClasses2 <- as.factor(ifelse(predictTestTwit2 > 0.5,1,0))
print(confusionMatrix(preictedClasses2,Test_twitter$party))
Model_glm3 <- glm(party ~ replies+retweets , family = "binomial", data = Train_twitter, maxit = 100)
predictTestTwit3 <- predict(Model_glm3, Test_twitter, type = "response")
preictedClasses3 <- as.factor(ifelse(predictTestTwit2 > 0.5,1,0))
print(confusionMatrix(preictedClasses3,Test_twitter$party))
gc()
gc()
library(dplyr)
library(tidyverse)
library(factoextra)
library(cluster)
library(gridExtra)
library(flexclust)
library(caret)
twtr <- read.csv("twitter_data_01.csv")
# convert party feature to factor
twtr$party <- as.factor(twtr$party)
twtr1 <- twtr[sample(nrow(twtr), 30000),]
twtr1 <- twtr1[,c(4,5,6,8)]
xtabs(~party,data = twtr1)
#summary(twtr.km)
set.seed(420)
# Model for partitioning twtr dataset into training and test data (70-30 split)
partModel <- createDataPartition(twtr1$party, p=0.7, list = FALSE)
# Creating the training and test datasets by applying the model
twTrain <- twtr1[partModel,]
twTest <- twtr1[-partModel,]
# create the logistic regression model
glmModel <- glm(party ~ . , family = "binomial", data = twTrain, maxit = 100)
summary(glmModel)
# prediction on the training set
predictTw <- predict(glmModel, twTrain, type = "response")
# assigning cutoff values
predictTw <- as.factor(ifelse(predictTw > 0.5,1,0))
print(confusionMatrix(predictTw,twTrain$party))
# prediction on the test set
predictTwTest <- predict(glmModel, twTest, type = "response")
# assigning cutoff values
predictTwTest <- as.factor(ifelse(predictTwTest > 0.5,1,0))
print(confusionMatrix(predictTwTest,twTest$party))
set.seed(420)
#twtr0 <- twtr[sample(nrow(twtr), 5000),]
twtr.km <- twtr1[,c(1,2,3)]
twtr.km <- scale(twtr.km) # scaling the dataset using z-score
#distance <- get_dist(twtr.km, method = "euclidean")
#fviz_dist(distance, gradient = list(low="dark green", mid="white", high="red"))
#head(round(as.matrix(distance),2),4) # displaying the first four rows rounded to 2 decimal places
set.seed(420)
# applying the Silhouette method using the fviz_nbclust function
fviz_nbclust(twtr.km, kmeans, method = "silhouette")
set.seed(420)
k2 <- kmeans(twtr.km, centers = 2, nstart = 50)
str(k2)
table(k2$cluster)
fviz_cluster(k2, data = twtr.km, pointsize = 1, labelsize = 7)
#incorporating the clusters with the non selected features and append to dataset
temp <- as.data.frame(list(k2$cluster))
twtr1$cluster <- temp[,c(1)]
twtr1$cluster <- as.factor(twtr1$cluster)
twtr1[,5] <- as.numeric(as.character(twtr1[,5]))
set.seed(420)
# Model for partitioning twtr dataset into training and test data (70-30 split)
T_idx <- createDataPartition(twtr1$party, p=0.7, list = FALSE)
# Creating the training and test datasets by applying the model
Train_twitter <- twtr1[T_idx,]
Test_twitter <- twtr1[-T_idx,]
summary(Train_twitter)
summary(Test_twitter)
# create the logistic regression model
Model_glm <- glm(party ~ . , family = "binomial", data = Train_twitter, maxit = 100)
# prediction on the test set
predictTestTwit <- predict(Model_glm, Test_twitter, type = "response")
# assigning cutoff values
preictedClasses <- as.factor(ifelse(predictTestTwit > 0.5,1,0))
# generating a confusion matrix and related details from the predictions
# made by the model
print(confusionMatrix(preictedClasses,Test_twitter$party))
# looking at ANOVA result or the model itself, we can see that retweets do not
# have a significant influence on the prediction; dropping it may give us a
# better predictive model maybe justify it
Model_glm2 <- glm(party ~ replies+retweets+cluster , family = "binomial", data = Train_twitter, maxit = 100)
predictTestTwit2 <- predict(Model_glm2, Test_twitter, type = "response")
preictedClasses2 <- as.factor(ifelse(predictTestTwit2 > 0.5,1,0))
print(confusionMatrix(preictedClasses2,Test_twitter$party))
Model_glm3 <- glm(party ~ replies+retweets , family = "binomial", data = Train_twitter, maxit = 100)
predictTestTwit3 <- predict(Model_glm3, Test_twitter, type = "response")
preictedClasses3 <- as.factor(ifelse(predictTestTwit2 > 0.5,1,0))
print(confusionMatrix(preictedClasses3,Test_twitter$party))
predicted.data <- data.frame(probability.of.party=Model_glm$fitted.values, party=Test_twitter$party)
predicted.data <- data.frame(probability.of.party=Model_glm$fitted.values, party=Train_twitter$party)
predicted.data <- predicted.data[order(predicted.data$probability.of.party, decreasing = FALSE),]
predicted.data$rank <- 1:nrow(predicted.data)
library(ggplot2)
library(cowplot)
ggplot(data=predicted.data, aes(x=rank, y=probability.of.party)) + xlab("Index") + ylab("Predicted prob of party")
ggplot(data=predicted.data, aes(x=rank, y=probability.of.party)) + geom_point(aes(color=party), alpha=1, shape=4, stroke=2) + xlab("Index") + ylab("Predicted prob of party")
str(twtr)
str(twtr1)
summary(twtr)
gc()
gc()
gc()
library(dplyr)
library(tidyverse)
library(factoextra)
library(cluster)
library(gridExtra)
library(flexclust)
library(caret)
twtr <- read.csv("twitter_data_01.csv")
# convert party feature to factor
twtr$party <- as.factor(twtr$party)
twtr1 <- twtr
#twtr1 <- twtr[sample(nrow(twtr), 30000),]
twtr1 <- twtr1[,c(4,5,6,8)]
xtabs(~party,data = twtr1)
#summary(twtr.km)
set.seed(420)
# Model for partitioning twtr dataset into training and test data (70-30 split)
partModel <- createDataPartition(twtr1$party, p=0.7, list = FALSE)
# Creating the training and test datasets by applying the model
twTrain <- twtr1[partModel,]
twTest <- twtr1[-partModel,]
# create the logistic regression model
glmModel <- glm(party ~ . , family = "binomial", data = twTrain, maxit = 100)
summary(glmModel)
# prediction on the training set
predictTw <- predict(glmModel, twTrain, type = "response")
# assigning cutoff values
predictTw <- as.factor(ifelse(predictTw > 0.5,1,0))
print(confusionMatrix(predictTw,twTrain$party))
# prediction on the test set
predictTwTest <- predict(glmModel, twTest, type = "response")
# assigning cutoff values
predictTwTest <- as.factor(ifelse(predictTwTest > 0.5,1,0))
print(confusionMatrix(predictTwTest,twTest$party))
set.seed(420)
#twtr0 <- twtr[sample(nrow(twtr), 5000),]
twtr.km <- twtr1[,c(1,2,3)]
twtr.km <- scale(twtr.km) # scaling the dataset using z-score
#distance <- get_dist(twtr.km, method = "euclidean")
#fviz_dist(distance, gradient = list(low="dark green", mid="white", high="red"))
#head(round(as.matrix(distance),2),4) # displaying the first four rows rounded to 2 decimal places
set.seed(420)
# applying the Silhouette method using the fviz_nbclust function
fviz_nbclust(twtr.km, kmeans, method = "silhouette")
set.seed(420)
k2 <- kmeans(twtr.km, centers = 2, nstart = 50)
str(k2)
table(k2$cluster)
fviz_cluster(k2, data = twtr.km, pointsize = 1, labelsize = 7)
#incorporating the clusters with the non selected features and append to dataset
temp <- as.data.frame(list(k2$cluster))
twtr1$cluster <- temp[,c(1)]
twtr1$cluster <- as.factor(twtr1$cluster)
twtr1[,5] <- as.numeric(as.character(twtr1[,5]))
set.seed(420)
# Model for partitioning twtr dataset into training and test data (70-30 split)
T_idx <- createDataPartition(twtr1$party, p=0.7, list = FALSE)
# Creating the training and test datasets by applying the model
Train_twitter <- twtr1[T_idx,]
Test_twitter <- twtr1[-T_idx,]
summary(Train_twitter)
summary(Test_twitter)
# create the logistic regression model
Model_glm <- glm(party ~ . , family = "binomial", data = Train_twitter, maxit = 100)
# prediction on the test set
predictTestTwit <- predict(Model_glm, Test_twitter, type = "response")
# assigning cutoff values
preictedClasses <- as.factor(ifelse(predictTestTwit > 0.5,1,0))
# generating a confusion matrix and related details from the predictions
# made by the model
print(confusionMatrix(preictedClasses,Test_twitter$party))
# looking at ANOVA result or the model itself, we can see that retweets do not
# have a significant influence on the prediction; dropping it may give us a
# better predictive model maybe justify it
Model_glm2 <- glm(party ~ replies+retweets+cluster , family = "binomial", data = Train_twitter, maxit = 100)
predictTestTwit2 <- predict(Model_glm2, Test_twitter, type = "response")
preictedClasses2 <- as.factor(ifelse(predictTestTwit2 > 0.5,1,0))
print(confusionMatrix(preictedClasses2,Test_twitter$party))
Model_glm3 <- glm(party ~ replies+retweets , family = "binomial", data = Train_twitter, maxit = 100)
predictTestTwit3 <- predict(Model_glm3, Test_twitter, type = "response")
preictedClasses3 <- as.factor(ifelse(predictTestTwit2 > 0.5,1,0))
print(confusionMatrix(preictedClasses3,Test_twitter$party))
set.seed(420)
twtr.km2 <- twtr1[,c(1,2,3)]
# sampling 10000 datasets to avoid vector memory exhaustion
twtr.km2 <- twtr[sample(nrow(twtr.km2), 10000),]
# applying the Silhouette method using the fviz_nbclust function
fviz_nbclust(twtr.km2, kmeans, method = "silhouette")
set.seed(420)
twtr.km2 <- twtr1[,c(1,2,3)]
# sampling 10000 datasets to avoid vector memory exhaustion
twtr.km2 <- twtr[sample(nrow(twtr.km2), 10000),]
# applying the Silhouette method using the fviz_nbclust function
fviz_nbclust(twtr.km2, kmeans, method = "silhouette")
gc()
library(dplyr)
library(tidyverse)
library(factoextra)
library(cluster)
library(gridExtra)
library(flexclust)
library(caret)
twtr <- read.csv("twitter_data_01.csv")
# convert party feature to factor
twtr$party <- as.factor(twtr$party)
twtr1 <- twtr
#twtr1 <- twtr[sample(nrow(twtr), 30000),]
twtr1 <- twtr1[,c(4,5,6,8)]
xtabs(~party,data = twtr1)
#summary(twtr.km)
set.seed(420)
# Model for partitioning twtr dataset into training and test data (70-30 split)
partModel <- createDataPartition(twtr1$party, p=0.7, list = FALSE)
# Creating the training and test datasets by applying the model
twTrain <- twtr1[partModel,]
twTest <- twtr1[-partModel,]
# create the logistic regression model
glmModel <- glm(party ~ . , family = "binomial", data = twTrain, maxit = 100)
summary(glmModel)
# prediction on the training set
predictTw <- predict(glmModel, twTrain, type = "response")
# assigning cutoff values
predictTw <- as.factor(ifelse(predictTw > 0.5,1,0))
print(confusionMatrix(predictTw,twTrain$party))
# prediction on the test set
predictTwTest <- predict(glmModel, twTest, type = "response")
# assigning cutoff values
predictTwTest <- as.factor(ifelse(predictTwTest > 0.5,1,0))
print(confusionMatrix(predictTwTest,twTest$party))
set.seed(420)
#twtr0 <- twtr[sample(nrow(twtr), 5000),]
twtr.km <- twtr1[,c(1,2,3)]
twtr.km <- scale(twtr.km) # scaling the dataset using z-score
#distance <- get_dist(twtr.km, method = "euclidean")
#fviz_dist(distance, gradient = list(low="dark green", mid="white", high="red"))
#head(round(as.matrix(distance),2),4) # displaying the first four rows rounded to 2 decimal places
set.seed(420)
twtr.km2 <- twtr1[,c(1,2,3)]
# sampling 10000 datasets to avoid vector memory exhaustion
twtr.km2 <- twtr.km2[sample(nrow(twtr.km2), 10000),]
# applying the Silhouette method using the fviz_nbclust function
fviz_nbclust(twtr.km2, kmeans, method = "silhouette")
set.seed(420)
k2 <- kmeans(twtr.km, centers = 2, nstart = 50)
str(k2)
table(k2$cluster)
fviz_cluster(k2, data = twtr.km, pointsize = 1, labelsize = 7)
#incorporating the clusters with the non selected features and append to dataset
temp <- as.data.frame(list(k2$cluster))
twtr1$cluster <- temp[,c(1)]
twtr1$cluster <- as.factor(twtr1$cluster)
twtr1[,5] <- as.numeric(as.character(twtr1[,5]))
set.seed(420)
# Model for partitioning twtr dataset into training and test data (70-30 split)
T_idx <- createDataPartition(twtr1$party, p=0.7, list = FALSE)
# Creating the training and test datasets by applying the model
Train_twitter <- twtr1[T_idx,]
Test_twitter <- twtr1[-T_idx,]
summary(Train_twitter)
summary(Test_twitter)
# create the logistic regression model
Model_glm <- glm(party ~ . , family = "binomial", data = Train_twitter, maxit = 100)
# prediction on the test set
predictTestTwit <- predict(Model_glm, Test_twitter, type = "response")
# assigning cutoff values
preictedClasses <- as.factor(ifelse(predictTestTwit > 0.5,1,0))
# generating a confusion matrix and related details from the predictions
# made by the model
print(confusionMatrix(preictedClasses,Test_twitter$party))
# looking at ANOVA result or the model itself, we can see that retweets do not
# have a significant influence on the prediction; dropping it may give us a
# better predictive model maybe justify it
Model_glm2 <- glm(party ~ replies+retweets+cluster , family = "binomial", data = Train_twitter, maxit = 100)
predictTestTwit2 <- predict(Model_glm2, Test_twitter, type = "response")
preictedClasses2 <- as.factor(ifelse(predictTestTwit2 > 0.5,1,0))
print(confusionMatrix(preictedClasses2,Test_twitter$party))
Model_glm3 <- glm(party ~ replies+retweets , family = "binomial", data = Train_twitter, maxit = 100)
predictTestTwit3 <- predict(Model_glm3, Test_twitter, type = "response")
preictedClasses3 <- as.factor(ifelse(predictTestTwit2 > 0.5,1,0))
print(confusionMatrix(preictedClasses3,Test_twitter$party))
summary(Model_glm)
ll.null <- Model_glm$null.deviance/-2
ll.proposed <- Model_glm$deviance/-2
ll.null - ll.proposed) / ll.null
(ll.null - ll.proposed) / ll.null
1 - pchisq(2*(ll.proposed -ll.null), df = (length(Model_glm$coefficients)-1))
predicted.data <- data.frame(probability.of.party=Model_glm$fitted.values, party=Train_twitter$party)
predicted.data <- predicted.data[order(predicted.data$probability.of.party, decreasing = FALSE),]
predicted.data$rank <- 1:nrow(predicted.data)
library(ggplot2)
library(cowplot)
ggplot(data=predicted.data, aes(x=rank, y=probability.of.party)) + xlab("Index") + ylab("Predicted prob of party")
ggplot(data=predicted.data, aes(x=rank, y=probability.of.party)) + geom_point(aes(color=party), alpha=1, shape=4, stroke=2) + xlab("Index") + ylab("Predicted prob of party")
predicted.data <- data.frame(probability.of.party=Model_glm$fitted.values, party=Train_twitter$party)
predicted.data <- predicted.data[order(predicted.data$probability.of.party, decreasing = FALSE),]
predicted.data$rank <- 1:nrow(predicted.data)
library(ggplot2)
library(cowplot)
ggplot(data=predicted.data, aes(x=rank, y=probability.of.party)) + xlab("Index") + ylab("Predicted prob of party")
ggplot(data=predicted.data, aes(x=rank, y=probability.of.party)) + geom_point(aes(color=party), alpha=1, shape=4, stroke=2) + xlab("Index") + ylab("Predicted prob of party")
predicted.data <- data.frame(probability.of.party=Model_glm$fitted.values, party=Train_twitter$party)
predicted.data <- predicted.data[order(predicted.data$probability.of.party, decreasing = FALSE),]
predicted.data$rank <- 1:nrow(predicted.data)
library(ggplot2)
library(cowplot)
ggplot(data=predicted.data, aes(x=rank, y=probability.of.party)) + xlab("Index") + ylab("Predicted prob of party")
ggplot(data=predicted.data, aes(x=rank, y=probability.of.party)) + geom_point(aes(color=party), alpha=1, shape=4, stroke=2) + xlab("Index") + ylab("Predicted prob of party")
predicted.data <- data.frame(probability.of.party=Model_glm$fitted.values, party=Train_twitter$party)
predicted.data <- predicted.data[order(predicted.data$probability.of.party, decreasing = FALSE),]
predicted.data$rank <- 1:nrow(predicted.data)
library(ggplot2)
library(cowplot)
ggplot(data=predicted.data, aes(x=rank, y=probability.of.party)) + xlab("Index") + ylab("Predicted prob of party")
ggplot(data=predicted.data, aes(x=rank, y=probability.of.party)) + geom_point(aes(color=party), alpha=1, shape=4, stroke=2) + xlab("Index") + ylab("Predicted prob of party")
plot(party ~ ., data=Train_twitter, col = "steelblue")
lines(party ~ ., Train_twitter, lwd=2)
# load relevant libraries and explore data
library(dplyr)
library(tidyverse)
library(factoextra)
library(cluster)
library(gridExtra)
library(flexclust)
library(caret)
twtr <- read.csv("twitter_data_01.csv")
# convert party feature to factor
twtr$party <- as.factor(twtr$party)
twtr1 <- twtr
summary(twtr1)
twtr1 <- twtr1[,c(4,5,6,8)]
xtabs(~party,data = twtr1)
# load relevant libraries and explore data
library(dplyr)
library(tidyverse)
library(factoextra)
library(cluster)
library(gridExtra)
library(flexclust)
library(caret)
twtr <- read.csv("twitter_data_01.csv")
# convert party feature to factor
twtr$party <- as.factor(twtr$party)
twtr1 <- twtr
str(twtr1)
twtr1 <- twtr1[,c(4,5,6,8)]
xtabs(~party,data = twtr1)
xtabs(~party,data = twtr1)
