set.seed(420)
distance <- get_dist(twtr.km, method = "euclidean")
#fviz_dist(distance, gradient = list(low="dark green", mid="white", high="red"))
#head(round(as.matrix(distance),2),4) # displaying the first four rows rounded to 2 decimal places
set.seed(420)
# applying the Silhouette method using the fviz_nbclust function
#fviz_nbclust(twtr.km, kmeans, method = "silhouette")
fviz_nbclust(twtr.km, kmeans, method = "wss")
gc()
gc()
library(tidyverse)
library(factoextra)
library(dplyr)
library(cluster)
library(gridExtra)
library(flexclust)
library(caret)
twtr <- read.csv("twitter_data.csv")
# convert party feature to factor
twtr$party <- as.factor(twtr$party)
twtr0 <- twtr[sample(nrow(twtr), 40000),]
twtr.km <- twtr0[,c(4,5,6)] # select the numeric variables only
twtr.km <- scale(twtr.km) # scaling the dataset using z-score
#check distribution of dependent variables
xtabs(~party,data = twtr0)
summary(twtr.km)
set.seed(420)
distance <- get_dist(twtr.km, method = "euclidean")
#fviz_dist(distance, gradient = list(low="dark green", mid="white", high="red"))
#head(round(as.matrix(distance),2),4) # displaying the first four rows rounded to 2 decimal places
set.seed(420)
# applying the Silhouette method using the fviz_nbclust function
fviz_nbclust(twtr.km, kmeans, method = "silhouette")
#fviz_nbclust(twtr.km, kmeans, method = "wss")
#grid.arrange(g_sil,g_wss)
gc()
gc()
gc()
gc()
setwd("~/Documents/KSU-MSBA/64060_egebrese/Final Exam")
gc()
library(dplyr)
library(tidyverse)
library(factoextra)
library(cluster)
library(gridExtra)
library(flexclust)
library(caret)
twtr <- read.csv("twitter_data_01.csv")
# convert party feature to factor
twtr$party <- as.factor(twtr$party)
twtr1 <- twtr[sample(nrow(twtr), 30000),]
twtr1 <- twtr1[,c(4,5,6,8)]
xtabs(~party,data = twtr1)
#summary(twtr.km)
set.seed(420)
# Model for partitioning twtr dataset into training and test data (70-30 split)
partModel <- createDataPartition(twtr1$party, p=0.7, list = FALSE)
# Creating the training and test datasets by applying the model
twTrain <- twtr1[partModel,]
twTest <- twtr1[-partModel,]
# create the logistic regression model
glmModel <- glm(party ~ . , family = "binomial", data = twTrain, maxit = 100)
summary(glmModel)
# prediction on the training set
predictTw <- predict(glmModel, twTrain, type = "response")
# assigning cutoff values
predictTw <- as.factor(ifelse(predictTw > 0.5,1,0))
print(confusionMatrix(predictTw,twTrain$party))
# prediction on the test set
predictTwTest <- predict(glmModel, twTest, type = "response")
# assigning cutoff values
predictTwTest <- as.factor(ifelse(predictTwTest > 0.5,1,0))
print(confusionMatrix(predictTwTest,twTest$party))
set.seed(420)
# applying the Silhouette method using the fviz_nbclust function
fviz_nbclust(twtr.km, kmeans, method = "silhouette")
set.seed(420)
#twtr0 <- twtr[sample(nrow(twtr), 5000),]
twtr.km <- twtr1[,c(1,2,3)]
twtr.km <- scale(twtr.km) # scaling the dataset using z-score
#distance <- get_dist(twtr.km, method = "euclidean")
#fviz_dist(distance, gradient = list(low="dark green", mid="white", high="red"))
#head(round(as.matrix(distance),2),4) # displaying the first four rows rounded to 2 decimal places
set.seed(420)
# applying the Silhouette method using the fviz_nbclust function
fviz_nbclust(twtr.km, kmeans, method = "silhouette")
set.seed(420)
k2 <- kmeans(twtr.km, centers = 2, nstart = 50)
str(k2)
table(k2$cluster)
fviz_cluster(k2, data = twtr.km, pointsize = 1, labelsize = 7)
#incorporating the clusters with the non selected features and append to dataset
temp <- as.data.frame(list(k2$cluster))
twtr1$cluster <- temp[,c(1)]
twtr1$cluster <- as.factor(twtr1$cluster)
twtr1[,5] <- as.numeric(as.character(twtr1[,5]))
set.seed(420)
# Model for partitioning twtr dataset into training and test data (70-30 split)
T_idx <- createDataPartition(twtr1$party, p=0.7, list = FALSE)
# Creating the training and test datasets by applying the model
Train_twitter <- twtr1[T_idx,]
Test_twitter <- twtr1[-T_idx,]
summary(Train_twitter)
summary(Test_twitter)
# create the logistic regression model
Model_glm <- glm(party ~ . , family = "binomial", data = Train_twitter, maxit = 100)
# prediction on the test set
predictTestTwit <- predict(Model_glm, Test_twitter, type = "response")
# assigning cutoff values
preictedClasses <- as.factor(ifelse(predictTestTwit > 0.5,1,0))
# generating a confusion matrix and related details from the predictions
# made by the model
print(confusionMatrix(preictedClasses,Test_twitter$party))
# looking at ANOVA result or the model itself, we can see that retweets do not
# have a significant influence on the prediction; dropping it may give us a
# better predictive model maybe justify it
Model_glm2 <- glm(party ~ replies+retweets+cluster , family = "binomial", data = Train_twitter, maxit = 100)
predictTestTwit2 <- predict(Model_glm2, Test_twitter, type = "response")
preictedClasses2 <- as.factor(ifelse(predictTestTwit2 > 0.5,1,0))
print(confusionMatrix(preictedClasses2,Test_twitter$party))
Model_glm3 <- glm(party ~ replies+retweets , family = "binomial", data = Train_twitter, maxit = 100)
predictTestTwit3 <- predict(Model_glm3, Test_twitter, type = "response")
preictedClasses3 <- as.factor(ifelse(predictTestTwit2 > 0.5,1,0))
print(confusionMatrix(preictedClasses3,Test_twitter$party))
gc()
library(dplyr)
library(tidyverse)
library(factoextra)
library(cluster)
library(gridExtra)
library(flexclust)
library(caret)
twtr <- read.csv("twitter_data_01.csv")
# convert party feature to factor
twtr$party <- as.factor(twtr$party)
twtr1 <- twtr[sample(nrow(twtr), 50000),]
twtr1 <- twtr1[,c(4,5,6,8)]
xtabs(~party,data = twtr1)
#summary(twtr.km)
set.seed(420)
# Model for partitioning twtr dataset into training and test data (70-30 split)
partModel <- createDataPartition(twtr1$party, p=0.7, list = FALSE)
# Creating the training and test datasets by applying the model
twTrain <- twtr1[partModel,]
twTest <- twtr1[-partModel,]
# create the logistic regression model
glmModel <- glm(party ~ . , family = "binomial", data = twTrain, maxit = 100)
summary(glmModel)
# prediction on the training set
predictTw <- predict(glmModel, twTrain, type = "response")
# assigning cutoff values
predictTw <- as.factor(ifelse(predictTw > 0.5,1,0))
print(confusionMatrix(predictTw,twTrain$party))
# prediction on the test set
predictTwTest <- predict(glmModel, twTest, type = "response")
# assigning cutoff values
predictTwTest <- as.factor(ifelse(predictTwTest > 0.5,1,0))
print(confusionMatrix(predictTwTest,twTest$party))
set.seed(420)
#twtr0 <- twtr[sample(nrow(twtr), 5000),]
twtr.km <- twtr1[,c(1,2,3)]
twtr.km <- scale(twtr.km) # scaling the dataset using z-score
#distance <- get_dist(twtr.km, method = "euclidean")
#fviz_dist(distance, gradient = list(low="dark green", mid="white", high="red"))
#head(round(as.matrix(distance),2),4) # displaying the first four rows rounded to 2 decimal places
set.seed(420)
# applying the Silhouette method using the fviz_nbclust function
fviz_nbclust(twtr.km, kmeans, method = "silhouette")
set.seed(420)
k2 <- kmeans(twtr.km, centers = 2, nstart = 50)
str(k2)
table(k2$cluster)
fviz_cluster(k2, data = twtr.km, pointsize = 1, labelsize = 7)
#incorporating the clusters with the non selected features and append to dataset
temp <- as.data.frame(list(k2$cluster))
twtr1$cluster <- temp[,c(1)]
twtr1$cluster <- as.factor(twtr1$cluster)
twtr1[,5] <- as.numeric(as.character(twtr1[,5]))
set.seed(420)
# Model for partitioning twtr dataset into training and test data (70-30 split)
T_idx <- createDataPartition(twtr1$party, p=0.7, list = FALSE)
# Creating the training and test datasets by applying the model
Train_twitter <- twtr1[T_idx,]
Test_twitter <- twtr1[-T_idx,]
summary(Train_twitter)
summary(Test_twitter)
# create the logistic regression model
Model_glm <- glm(party ~ . , family = "binomial", data = Train_twitter, maxit = 100)
# prediction on the test set
predictTestTwit <- predict(Model_glm, Test_twitter, type = "response")
# assigning cutoff values
preictedClasses <- as.factor(ifelse(predictTestTwit > 0.5,1,0))
# generating a confusion matrix and related details from the predictions
# made by the model
print(confusionMatrix(preictedClasses,Test_twitter$party))
# looking at ANOVA result or the model itself, we can see that retweets do not
# have a significant influence on the prediction; dropping it may give us a
# better predictive model maybe justify it
Model_glm2 <- glm(party ~ replies+retweets+cluster , family = "binomial", data = Train_twitter, maxit = 100)
predictTestTwit2 <- predict(Model_glm2, Test_twitter, type = "response")
preictedClasses2 <- as.factor(ifelse(predictTestTwit2 > 0.5,1,0))
print(confusionMatrix(preictedClasses2,Test_twitter$party))
Model_glm3 <- glm(party ~ replies+retweets , family = "binomial", data = Train_twitter, maxit = 100)
predictTestTwit3 <- predict(Model_glm3, Test_twitter, type = "response")
preictedClasses3 <- as.factor(ifelse(predictTestTwit2 > 0.5,1,0))
print(confusionMatrix(preictedClasses3,Test_twitter$party))
summary(Model_glm)
summary(Model_glm2)
summary(Model_glm3)
gc()
gc()
gc()
library(dplyr)
library(tidyverse)
library(factoextra)
library(cluster)
library(gridExtra)
library(flexclust)
library(caret)
twtr <- read.csv("twitter_data_01.csv")
# convert party feature to factor
twtr$party <- as.factor(twtr$party)
twtr1 <- twtr[sample(nrow(twtr), 50000),]
twtr1 <- twtr1[,c(4,5,6,8)]
xtabs(~party,data = twtr1)
#summary(twtr.km)
set.seed(420)
# Model for partitioning twtr dataset into training and test data (70-30 split)
partModel <- createDataPartition(twtr1$party, p=0.7, list = FALSE)
# Creating the training and test datasets by applying the model
twTrain <- twtr1[partModel,]
twTest <- twtr1[-partModel,]
# create the logistic regression model
glmModel <- glm(party ~ . , family = "binomial", data = twTrain, maxit = 100)
summary(glmModel)
# prediction on the training set
predictTw <- predict(glmModel, twTrain, type = "response")
# assigning cutoff values
predictTw <- as.factor(ifelse(predictTw > 0.5,1,0))
print(confusionMatrix(predictTw,twTrain$party))
# prediction on the test set
predictTwTest <- predict(glmModel, twTest, type = "response")
# assigning cutoff values
predictTwTest <- as.factor(ifelse(predictTwTest > 0.5,1,0))
print(confusionMatrix(predictTwTest,twTest$party))
set.seed(420)
#twtr0 <- twtr[sample(nrow(twtr), 5000),]
twtr.km <- twtr1[,c(1,2,3)]
twtr.km <- scale(twtr.km) # scaling the dataset using z-score
#distance <- get_dist(twtr.km, method = "euclidean")
#fviz_dist(distance, gradient = list(low="dark green", mid="white", high="red"))
#head(round(as.matrix(distance),2),4) # displaying the first four rows rounded to 2 decimal places
set.seed(420)
k2 <- kmeans(twtr.km, centers = 2, nstart = 50)
str(k2)
table(k2$cluster)
fviz_cluster(k2, data = twtr.km, pointsize = 1, labelsize = 7)
#incorporating the clusters with the non selected features and append to dataset
temp <- as.data.frame(list(k2$cluster))
twtr1$cluster <- temp[,c(1)]
twtr1$cluster <- as.factor(twtr1$cluster)
twtr1[,5] <- as.numeric(as.character(twtr1[,5]))
set.seed(420)
# Model for partitioning twtr dataset into training and test data (70-30 split)
T_idx <- createDataPartition(twtr1$party, p=0.7, list = FALSE)
# Creating the training and test datasets by applying the model
Train_twitter <- twtr1[T_idx,]
Test_twitter <- twtr1[-T_idx,]
summary(Train_twitter)
summary(Test_twitter)
# create the logistic regression model
Model_glm <- glm(party ~ . , family = "binomial", data = Train_twitter, maxit = 100)
# prediction on the test set
predictTestTwit <- predict(Model_glm, Test_twitter, type = "response")
# assigning cutoff values
preictedClasses <- as.factor(ifelse(predictTestTwit > 0.5,1,0))
# generating a confusion matrix and related details from the predictions
# made by the model
print(confusionMatrix(preictedClasses,Test_twitter$party))
# looking at ANOVA result or the model itself, we can see that retweets do not
# have a significant influence on the prediction; dropping it may give us a
# better predictive model maybe justify it
Model_glm2 <- glm(party ~ replies+retweets+cluster , family = "binomial", data = Train_twitter, maxit = 100)
predictTestTwit2 <- predict(Model_glm2, Test_twitter, type = "response")
preictedClasses2 <- as.factor(ifelse(predictTestTwit2 > 0.5,1,0))
print(confusionMatrix(preictedClasses2,Test_twitter$party))
Model_glm3 <- glm(party ~ replies+retweets , family = "binomial", data = Train_twitter, maxit = 100)
predictTestTwit3 <- predict(Model_glm3, Test_twitter, type = "response")
preictedClasses3 <- as.factor(ifelse(predictTestTwit2 > 0.5,1,0))
print(confusionMatrix(preictedClasses3,Test_twitter$party))
gc()
gc()
gc()
gc()
library(dplyr)
library(tidyverse)
library(factoextra)
library(cluster)
library(gridExtra)
library(flexclust)
library(caret)
twtr <- read.csv("twitter_data_01.csv")
# convert party feature to factor
twtr$party <- as.factor(twtr$party)
twtr1 <- twtr[sample(nrow(twtr), 50000),]
twtr1 <- twtr1[,c(4,5,6,8)]
xtabs(~party,data = twtr1)
#summary(twtr.km)
set.seed(420)
# Model for partitioning twtr dataset into training and test data (70-30 split)
partModel <- createDataPartition(twtr1$party, p=0.7, list = FALSE)
# Creating the training and test datasets by applying the model
twTrain <- twtr1[partModel,]
twTest <- twtr1[-partModel,]
# create the logistic regression model
glmModel <- glm(party ~ . , family = "binomial", data = twTrain, maxit = 100)
summary(glmModel)
# prediction on the training set
predictTw <- predict(glmModel, twTrain, type = "response")
# assigning cutoff values
predictTw <- as.factor(ifelse(predictTw > 0.5,1,0))
print(confusionMatrix(predictTw,twTrain$party))
# prediction on the test set
predictTwTest <- predict(glmModel, twTest, type = "response")
# assigning cutoff values
predictTwTest <- as.factor(ifelse(predictTwTest > 0.5,1,0))
print(confusionMatrix(predictTwTest,twTest$party))
set.seed(420)
#twtr0 <- twtr[sample(nrow(twtr), 5000),]
twtr.km <- twtr1[,c(1,2,3)]
twtr.km <- scale(twtr.km) # scaling the dataset using z-score
#distance <- get_dist(twtr.km, method = "euclidean")
#fviz_dist(distance, gradient = list(low="dark green", mid="white", high="red"))
#head(round(as.matrix(distance),2),4) # displaying the first four rows rounded to 2 decimal places
set.seed(420)
# applying the Silhouette method using the fviz_nbclust function
fviz_nbclust(twtr.km, kmeans, method = "silhouette")
set.seed(420)
k2 <- kmeans(twtr.km, centers = 2, nstart = 50)
str(k2)
table(k2$cluster)
fviz_cluster(k2, data = twtr.km, pointsize = 1, labelsize = 7)
#incorporating the clusters with the non selected features and append to dataset
temp <- as.data.frame(list(k2$cluster))
twtr1$cluster <- temp[,c(1)]
twtr1$cluster <- as.factor(twtr1$cluster)
twtr1[,5] <- as.numeric(as.character(twtr1[,5]))
set.seed(420)
# Model for partitioning twtr dataset into training and test data (70-30 split)
T_idx <- createDataPartition(twtr1$party, p=0.7, list = FALSE)
# Creating the training and test datasets by applying the model
Train_twitter <- twtr1[T_idx,]
Test_twitter <- twtr1[-T_idx,]
summary(Train_twitter)
summary(Test_twitter)
# create the logistic regression model
Model_glm <- glm(party ~ . , family = "binomial", data = Train_twitter, maxit = 100)
# prediction on the test set
predictTestTwit <- predict(Model_glm, Test_twitter, type = "response")
# assigning cutoff values
preictedClasses <- as.factor(ifelse(predictTestTwit > 0.5,1,0))
# generating a confusion matrix and related details from the predictions
# made by the model
print(confusionMatrix(preictedClasses,Test_twitter$party))
# looking at ANOVA result or the model itself, we can see that retweets do not
# have a significant influence on the prediction; dropping it may give us a
# better predictive model maybe justify it
Model_glm2 <- glm(party ~ replies+retweets+cluster , family = "binomial", data = Train_twitter, maxit = 100)
predictTestTwit2 <- predict(Model_glm2, Test_twitter, type = "response")
preictedClasses2 <- as.factor(ifelse(predictTestTwit2 > 0.5,1,0))
print(confusionMatrix(preictedClasses2,Test_twitter$party))
Model_glm3 <- glm(party ~ replies+retweets , family = "binomial", data = Train_twitter, maxit = 100)
predictTestTwit3 <- predict(Model_glm3, Test_twitter, type = "response")
preictedClasses3 <- as.factor(ifelse(predictTestTwit2 > 0.5,1,0))
print(confusionMatrix(preictedClasses3,Test_twitter$party))
gc()
gc()
library(dplyr)
library(tidyverse)
library(factoextra)
library(cluster)
library(gridExtra)
library(flexclust)
library(caret)
twtr <- read.csv("twitter_data_01.csv")
# convert party feature to factor
twtr$party <- as.factor(twtr$party)
twtr1 <- twtr[sample(nrow(twtr), 50000),]
twtr1 <- twtr1[,c(4,5,6,8)]
xtabs(~party,data = twtr1)
#summary(twtr.km)
set.seed(420)
# Model for partitioning twtr dataset into training and test data (70-30 split)
partModel <- createDataPartition(twtr1$party, p=0.7, list = FALSE)
# Creating the training and test datasets by applying the model
twTrain <- twtr1[partModel,]
twTest <- twtr1[-partModel,]
# create the logistic regression model
glmModel <- glm(party ~ . , family = "binomial", data = twTrain, maxit = 100)
summary(glmModel)
# prediction on the training set
predictTw <- predict(glmModel, twTrain, type = "response")
# assigning cutoff values
predictTw <- as.factor(ifelse(predictTw > 0.5,1,0))
print(confusionMatrix(predictTw,twTrain$party))
# prediction on the test set
predictTwTest <- predict(glmModel, twTest, type = "response")
# assigning cutoff values
predictTwTest <- as.factor(ifelse(predictTwTest > 0.5,1,0))
print(confusionMatrix(predictTwTest,twTest$party))
set.seed(420)
#twtr0 <- twtr[sample(nrow(twtr), 5000),]
twtr.km <- twtr1[,c(1,2,3)]
twtr.km <- scale(twtr.km) # scaling the dataset using z-score
#distance <- get_dist(twtr.km, method = "euclidean")
#fviz_dist(distance, gradient = list(low="dark green", mid="white", high="red"))
#head(round(as.matrix(distance),2),4) # displaying the first four rows rounded to 2 decimal places
set.seed(420)
k2 <- kmeans(twtr.km, centers = 2, nstart = 50)
str(k2)
table(k2$cluster)
fviz_cluster(k2, data = twtr.km, pointsize = 1, labelsize = 7)
#incorporating the clusters with the non selected features and append to dataset
temp <- as.data.frame(list(k2$cluster))
twtr1$cluster <- temp[,c(1)]
twtr1$cluster <- as.factor(twtr1$cluster)
twtr1[,5] <- as.numeric(as.character(twtr1[,5]))
set.seed(420)
# Model for partitioning twtr dataset into training and test data (70-30 split)
T_idx <- createDataPartition(twtr1$party, p=0.7, list = FALSE)
# Creating the training and test datasets by applying the model
Train_twitter <- twtr1[T_idx,]
Test_twitter <- twtr1[-T_idx,]
summary(Train_twitter)
summary(Test_twitter)
# create the logistic regression model
Model_glm <- glm(party ~ . , family = "binomial", data = Train_twitter, maxit = 100)
# prediction on the test set
predictTestTwit <- predict(Model_glm, Test_twitter, type = "response")
# assigning cutoff values
preictedClasses <- as.factor(ifelse(predictTestTwit > 0.5,1,0))
# generating a confusion matrix and related details from the predictions
# made by the model
print(confusionMatrix(preictedClasses,Test_twitter$party))
# looking at ANOVA result or the model itself, we can see that retweets do not
# have a significant influence on the prediction; dropping it may give us a
# better predictive model maybe justify it
Model_glm2 <- glm(party ~ replies+retweets+cluster , family = "binomial", data = Train_twitter, maxit = 100)
predictTestTwit2 <- predict(Model_glm2, Test_twitter, type = "response")
preictedClasses2 <- as.factor(ifelse(predictTestTwit2 > 0.5,1,0))
print(confusionMatrix(preictedClasses2,Test_twitter$party))
Model_glm3 <- glm(party ~ replies+retweets , family = "binomial", data = Train_twitter, maxit = 100)
predictTestTwit3 <- predict(Model_glm3, Test_twitter, type = "response")
preictedClasses3 <- as.factor(ifelse(predictTestTwit2 > 0.5,1,0))
print(confusionMatrix(preictedClasses3,Test_twitter$party))
gc()
gc()
library(dplyr)
library(tidyverse)
library(factoextra)
library(cluster)
library(gridExtra)
library(flexclust)
library(caret)
twtr <- read.csv("twitter_data_01.csv")
# convert party feature to factor
twtr$party <- as.factor(twtr$party)
twtr1 <- twtr[sample(nrow(twtr), 50000),]
twtr1 <- twtr1[,c(4,5,6,8)]
xtabs(~party,data = twtr1)
#summary(twtr.km)
set.seed(420)
# Model for partitioning twtr dataset into training and test data (70-30 split)
partModel <- createDataPartition(twtr1$party, p=0.7, list = FALSE)
# Creating the training and test datasets by applying the model
twTrain <- twtr1[partModel,]
twTest <- twtr1[-partModel,]
# create the logistic regression model
glmModel <- glm(party ~ . , family = "binomial", data = twTrain, maxit = 100)
summary(glmModel)
# prediction on the training set
predictTw <- predict(glmModel, twTrain, type = "response")
# assigning cutoff values
predictTw <- as.factor(ifelse(predictTw > 0.5,1,0))
print(confusionMatrix(predictTw,twTrain$party))
# prediction on the test set
predictTwTest <- predict(glmModel, twTest, type = "response")
# assigning cutoff values
predictTwTest <- as.factor(ifelse(predictTwTest > 0.5,1,0))
print(confusionMatrix(predictTwTest,twTest$party))
install.packages("fpc")
library("dbscan")
library("fpc")
db <- dbscan::dbscan(twtr1[,c(1,2)], eps = 0.5, minPts = 5)
print(db)
library("dbscan")
library("fpc")
df <- twtr1[,c(1,2)]
db <- dbscan::dbscan(df, eps = 0.5, minPts = 5)
#print(db)
plot(db, df, main = "DBSCAN", frame = FALSE)
library("dbscan")
library("fpc")
df <- twtr1[,c(1,2)]
db <- dbscan::dbscan(df, eps = 0.5, minPts = 5)
#print(db)
plot(db, df, main = "DBSCAN", frame = FALSE)
db2 <- fpc::dbscan(df, eps = 0.15, MinPts = 5)
plot(db2, df, main = "DBSCAN", frame = FALSE )
gc()
gc()
